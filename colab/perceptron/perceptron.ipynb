{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAUwLWJZx-2K"
   },
   "source": [
    "# A Simple Perceptron Implementation\n",
    "This Perceptron implementation was originally inspired by the the version from \"Python Machine Learning by Sebastian Raschka 1st edition\" (ISBN-13: 978-1783555130), but it has been completely re-written so as to have almost nothing in common with it. It has been written to be more pedagogically accessible to less savvy programmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Packages\n",
    "Python comes with a dizzying number of packages for all sorts of things. If you are going to do data science with python you will likely need to know quite a few. In this implementation, we wil be using:\n",
    "\n",
    "- [Numpy](http://www.numpy.org/): A great mathematical package for n-dimentional arrays and mathy sorts of things.\n",
    "- [MatPlotLib](https://matplotlib.org/): A great matlab-like plotting library for visualization.\n",
    "- [Pandas](https://pandas.pydata.org/): A reduculously easy to use data loading, visualization, processing, etc.\n",
    "- [SKLearn](https://scikit-learn.org/stable/): A package specifically for learning meachine learning.\n",
    "\n",
    "In order to use these packages, they first must be installed on your system. Once installed, we need to \"import them\". Read the comments in the code to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lu1MhA5cxeG8"
   },
   "outputs": [],
   "source": [
    "# Because we are lazy, I mean efficient, we don't want to type \"numpy\"\n",
    "# when ever we want to use something in the numpy package so we give\n",
    "# it a nickname \"np\" by importing it \"as np\". Thereafter whereever we\n",
    "# would have to type five letters (numpy) we can type two (np) being\n",
    "# 60% more efficient!\n",
    "import numpy as np\n",
    "# We can even give packages inside packages nicknames the same way.\n",
    "# Since we're only going to use the pyplot package inside the\n",
    "# matplotlib package, we can create \"plt\" pointing to it directly.\n",
    "import matplotlib.pyplot as plt\n",
    "# Same for pandas...\n",
    "import pandas as pd\n",
    "# Since we're only going to use the iris data set from sklearn,\n",
    "# we import ony the \"load_iris\" method here. We can now call\n",
    "# it directly\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "Here we load the data that we're going to use into a variable called\n",
    "\"`iris`\".\n",
    "\n",
    "The format is one that is a `dict` and has the format:\n",
    "```python\n",
    "  {\n",
    "    'data': array([ [...], [...], ... [...]],\n",
    "    'target': array([0,0,0, ... 2,2,2],\n",
    "    'target_names': array(['setosa', 'versicolor', 'virginica']),\n",
    "    'DESCR': 'Long string description',\n",
    "    'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
    "  }\n",
    "```\n",
    "Note that the `data`, `target`, and `target_names` keys have values that are numpy.nd arrays.\n",
    "This allows us to use all of the great NumPy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Wb5NOEpn7_WF",
    "outputId": "891e8640-fdce-40b0-94cf-2eaccdb02e90"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# Uncomment to see what this looks like.\n",
    "# print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data Data Sciency-Style\n",
    "\n",
    "Although the data format of `iris` is useful, we want it in a slightly different format. There are many ways we could use to get to the desired formats, but I'll show you a neat trick to load them into a Pandas data object that allows us to view the data nicely.\n",
    "\n",
    "**NOTE**: You don't need to know the details of this, but it's neat to see what Pandas and NumpPy can do for you in a single lines of code. If you would like an explanation of what is going on or an example of how to do this in a more \"long-hand\" intuitive way, just let me know and we'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see why this might be useful, especially to make sure that you have the data you think you have.\n",
    "\n",
    "**Note**: The first column with no heading and just numbers is not really a column in the data, it just shows you the index of each row with the index starting at `0` as in most languages. The data contain 150 rows (fifty for each species) indexed from `0` to `149` and 5 columns index from `0` to `4`.\n",
    "\n",
    "Moving along...\n",
    "\n",
    "# Getting the Features and Labels (Targets)\n",
    "\n",
    "Ultimately we want a 2-dimensional array of features, since our Perceptron implementation will only be able to deal with two. The `iris` data set also contain three species of flowers: setosa, versicolor, and virginica. The Perceptron is a binary classifier and therefore can only handle two, so we also need to take only two species.\n",
    "\n",
    "For this exercise, we'll take the first two: setosa and versicolor.\n",
    "\n",
    "We \"know\" that the first 100 rows contain those two species, because we looked at the data (trust me) even though we could use some tricks to extract the first two labels (aka targets). For now, we'll use our knowledge of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy Array Indexing\n",
    "\n",
    "Here we use \"fancy indexing\" which most array-like objects support. To grab the first 3 rows of a one dimensional array, we can do this:\n",
    "\n",
    "```python\n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(a[0:2])\n",
    "```\n",
    "Which will print `[1,2,3]`. You can also do things like `a[:2]` for the same result, `a[-1]` and much more. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(\"a = %s\" %a)\n",
    "print(\"a[0:2]  = %s\" %a[0:2])\n",
    "print(\"a[:2]   = %s\" %a[:2])\n",
    "print(\"a[-1]   = %s\" %a[-1])\n",
    "print(\"a[7:]   = %s\" %a[7:])\n",
    "print(\"a[7:10] = %s\" %a[7:10])\n",
    "print(\"a[7:-1] = %s\" %a[7:-1])\n",
    "print(\"a[:]    = %s\" %a[:])\n",
    "print(\"a[::2]  = %s\" %a[::2])\n",
    "print(\"a[1::2] = %s\" %a[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays go even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)\n",
    "print(a[a % 3 != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Labels (Targets)\n",
    "\n",
    "To get the labels (aka targets) from the pandas `dataset` for just the setosa and versicolor species, we'll grab the fith column (index `4`) for the first 100 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eo7XtB249E8l"
   },
   "outputs": [],
   "source": [
    "labels = dataset.iloc[0:100, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we did what we thought we did by printing the first 3 and last 3 elements of `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[:3],labels[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the labels are `floats`. Remember that the Perceptron does not understand text labels, only numerical ones, so this is what we want.\n",
    "\n",
    "We can also find out how many unique values there are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two values only, great! There are many neat things that we can do with NumPy arrays, by the way. If we want to count the number of non-zero values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to grab all of the rows with zero values, there are several ways to do this, but one great way is to mask the array. You can do that with a simple comparison operator. `labels == 0` will return an array of True and False and if you then use that to index into a NumPy array it will return only those values that have `True`. For example the following resturns the last 50 elements of `labels` since they have a value of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[labels == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method combinbed with the NumPy \"where\" method, we can convert all the values. Since we will first run a Perceptron with classes `-1` and `1` rather than `0` and `1`, we'll change all values accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(labels == 0, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now setosa is `-1` and versicolor is `1` (unchanged)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Features\n",
    "\n",
    "Now we'll get the features. Our poor little Perceptron binary classifier can only handle two, so we'll pick the sepal length and petal length.\n",
    "\n",
    "**Note**: Don't know what a sepal is? [Look it up](https://en.wikipedia.org/wiki/Sepal)! Don't know what a petal is? See me.\n",
    "\n",
    "Those are found in the first and third columns of our `dataset` (indexes `0` and `2` respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sDfFBYrB-jS_",
    "outputId": "44245f10-a659-408f-9b6c-b013556ab5a8"
   },
   "outputs": [],
   "source": [
    "features = dataset.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to make sure you got this right by using `head` and fancy indexing to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[:5])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good. We could have done this exact same thing in two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "labels, features = np.where(dataset.iloc[0:100, 4].values==0,-1,1), dataset.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I think you'll agree that it would have been harder to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "Classes and objects in python are a mostly like classes and object in other OO languages with some\n",
    "annyoing or welcome differences depending on your mood that day. I am going to assume that you\n",
    "understand the concepts of classes and objects well enough to read the code below and understand it\n",
    "especially with all the comments. If you find it confusing, just let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKRyRaj9xn5W"
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    A reasonably accurate but object-oriented and pythonesque\n",
    "    implementation of the 1957-1958 Rosenblat Perceptron.\n",
    "    \n",
    "    See:\n",
    "    \n",
    "    Rosenblatt, Frank. \"The perceptron: a probabilistic model for\n",
    "    information storage and organization in the brain.\" Psychological\n",
    "    review 65.6 (1958): 386.\n",
    "    \n",
    "    The __init__ function is omitted\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, features, labels, learning_rate=0.01, num_iterations=50, init_random=False, show_all=False):\n",
    "        \"\"\"\n",
    "        Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        features : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples\n",
    "            is the number of samples and n_features\n",
    "            is the number of features.\n",
    "        labels : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        learning_rate : float\n",
    "            Learning rate (between 0.0 and 1.0)\n",
    "        num_iterations: int\n",
    "            Number of passes or epochs over the training dataset.\n",
    "        init_rantom : boolean\n",
    "            If true, randomly initialize the weights, otherwise\n",
    "            make them all zero to start.\n",
    "        show_all : boolean\n",
    "            If true, will plot each iteration. Lots of output!\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "        # We \"store\" the values within the Perceptron for\n",
    "        # convenience. If you access these values they will always be\n",
    "        # the most recently \"fitted\" values, so keep that in mind. We\n",
    "        # do this so that we can later use the perceptron to visualize\n",
    "        # it. It would be better to have a different class to do that.\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        # Here we figure out what the two labels are. The labels need\n",
    "        # to be numeric values and could in theory be any to floating\n",
    "        # point numbers. Conventionally, they are either [0,1] or\n",
    "        # [-1,1].\n",
    "\n",
    "        # We can use the numpy unique method which returns a sorted np\n",
    "        # arry of all the unigue values. If the labels are not\n",
    "        # numbers, things may not work out too well.\n",
    "        unique_labels = np.unique(self.labels)\n",
    "        # Since this is a binary classifier, it can only handle two\n",
    "        # values. If we don't have two values, we die a horrible\n",
    "        # death.\n",
    "        if len(unique_labels) != 2:\n",
    "            raise ValueErrir(\"Sorry, there were %d unique labels in \"+\n",
    "                             \"the training set and we need exactly \"+\n",
    "                             \"two. Try avain.\" %len(unique_labels))\n",
    "        # The first value is the lowest\n",
    "        self.a_num = unique_labels[0]\n",
    "        self.b_num = unique_labels[1]\n",
    "        # The threshold is in the middle.\n",
    "        self.threshold = (self.a_num * 1.0 + self.b_num) / 2\n",
    "        # We'll use the rage to normalize the error we calculate.\n",
    "\n",
    "        # Create an array (list) of internal internal weights to be\n",
    "        # used by this perceptron. These \"internal\" weights are NOT\n",
    "        # the same thing as the weights between nodes in a neural\n",
    "        # network, but are similar in many ways.\n",
    "        \n",
    "        # We will need an array with one more member than the number\n",
    "        # of feathres since we will have a bias weight. To calculate\n",
    "        # the length of this array, we simply find the number of\n",
    "        # features (a 2D array and the size of the second dimension is\n",
    "        # the number of features) plus one.\n",
    "        num_weights = 1 + self.features.shape[1]\n",
    "        if init_random is False:\n",
    "          # If we're not going to initialize the internal weights\n",
    "          # randomly, then create an array (list) initialized to zero.\n",
    "          self.weights = np.zeros(num_weights)\n",
    "        else:\n",
    "          # Initialize the weights randomly.\n",
    "          self.weights = np.random.rand(num_weights)\n",
    "          # Note, that that numpy returns an array of randomm\n",
    "          # variables between 0.0 and 1.0. The values are not\n",
    "          # distributed in a Gaussian format, rather each value has\n",
    "          # approximately the same probability. For now, we do not go\n",
    "          # into the details, but this is generally not the best\n",
    "          # initialization possible. In order to make the values range\n",
    "          # from -1 to +1 (so we have a mix of positive and negative),\n",
    "          # we can stretch the values by two:\n",
    "          self.weights *= 2\n",
    "          # And then shift them down by one:\n",
    "          self.weights -= 1\n",
    "          # Now the values range from -1.0 to 1.0. Note that the\n",
    "          # values could be -1000 to +17, but since we're expecting\n",
    "          # values between -1 and 1 at the most, random weights\n",
    "          # between those two values seem most rasonable.\n",
    "          pass\n",
    "        # We will use an array (list) to keep track of the number of\n",
    "        # misclassified samples during each iteration (often referred\n",
    "        # to as \"epoch\"). Each iteration will have one value which is\n",
    "        # simply the count misclassified samples.\n",
    "        self.misclassifications_counter = []\n",
    "        # Now we run a loop self.num_iterations number of times. The\n",
    "        # no-name variable \"_\" is a convention used to tell the world\n",
    "        # that although the for loop requires us to use a variable, we\n",
    "        # will not be using it anywhere in the code.\n",
    "        for _ in range(self.num_iterations):\n",
    "            # The current number of misclassified samples\n",
    "            errors = 0\n",
    "            # Here we use the \"zip\" function to make things easier to\n",
    "            # code but with a cost to readability. In short,\n",
    "            # zip(self.features,self.labels) with iterate over all of\n",
    "            # the elements of self.features (a 2D array) and\n",
    "            # self.labels (a 1D array) and return a tuple in the form\n",
    "            # of the self.features element and the self.labels element\n",
    "            # at each index. self.features and self.labels must be the\n",
    "            # same length along the first dimension.\n",
    "            for sample_features, correct_label in zip(self.features, self.labels):\n",
    "                if show_all:\n",
    "                    self.show(True)\n",
    "                    pass\n",
    "\n",
    "                # For a data set where self.features has two features,\n",
    "                # sample_features = [feature1,feature2] and\n",
    "                # self.labels = correct_label class (a_num or n_num in\n",
    "                # our case)\n",
    "                \n",
    "                # We send along the two features and see which class\n",
    "                # we predict\n",
    "                predicted_label = self.predict_label(sample_features)\n",
    "                # Uncomment to see what's going on:\n",
    "                # print(\"Predicted = %s, Targed = %s\" %(predicted_label, correct_label))\n",
    "\n",
    "                # The difference between the correct and predicted\n",
    "                # tells us how far off we were. This could be positive\n",
    "                # or negative. Note that which you subtract from which is\n",
    "                # very important since the sign matters.\n",
    "                error_amount = predicted_label - correct_label\n",
    "\n",
    "                # We then multiply the difference by the learning rate\n",
    "                # which is between 0.0 and 1.0, meaning that we'll nudge\n",
    "                # the weights by a percentage of the difference.\n",
    "                nudge_amount = self.learning_rate * error_amount\n",
    "\n",
    "                #if _ < 1:\n",
    "                #    print(\"Predicted = %s, Crrect = %s, Error = %s, Nudge = %s\"\n",
    "                #          %(correct_label,predicted_label,error_amount, nudge_amount))\n",
    "                #    pass\n",
    "\n",
    "                # Now, for each of the weights associated with each\n",
    "                # feature, we update the weight incrementally by the\n",
    "                # nudge_amount value\n",
    "                self.weights[1:] += nudge_amount * sample_features\n",
    "\n",
    "                # We also need to update the bias weight. Since the\n",
    "                # bias input (you can think of it as the bias feature)\n",
    "                # is 1, we simply increment it by the update value\n",
    "                # (which again could be positive or negatiove)\n",
    "                self.weights[0] += nudge_amount\n",
    "                \n",
    "                # If error_amount was non-zero, then we misclassified, so\n",
    "                # cound it\n",
    "                errors += int(error_amount != 0.0)\n",
    "\n",
    "                # Close the loop for readability and continue with the\n",
    "                # next sample in the training set if any.\n",
    "                pass\n",
    "\n",
    "            # Append the error count to the\n",
    "            # self.misclassifications_counter array.\n",
    "            self.misclassifications_counter.append(errors)\n",
    "\n",
    "            # Now, because we know that if there were no errors the\n",
    "            # weights were not changed and therefore any subesquent\n",
    "            # iterations would also not change weights or results, we\n",
    "            # can stop if there were no errors and save some\n",
    "            # computing.\n",
    "            if(errors == 0):\n",
    "                return self\n",
    "\n",
    "            # If there were errors, we'll need to try another\n",
    "            # iteration (epoch) to see if we can improve. This method\n",
    "            # is \"stupid\" in that it is possible for a given data set\n",
    "            # that no solution will be found (there are ways to detect\n",
    "            # this problem, but not in this code) but it will continue\n",
    "            # blindly if there were misclassifications until it has\n",
    "            # completed the required number of iterations (epochs)\n",
    "            pass\n",
    "\n",
    "        # We're done, so return. I often return self rather than the\n",
    "        # implicit 'None' simply because it allows you to daisy chain\n",
    "        # calls. In this case I can't imagine why you would, but\n",
    "        # nevertheless...\n",
    "        return self\n",
    "    \n",
    "    def predict_label(self, features):\n",
    "        \"\"\"\n",
    "        \"Predict\" the class lable and return it. It will return a_num or b_num based\n",
    "        on our implementation of the classical Perceptron.\n",
    "        \"\"\"\n",
    "        # Get the float predicted label and theshold it to make this binary\n",
    "        predicted = np.where(self.predict_float(features) >= self.threshold, self.a_num, self.b_num)\n",
    "        return predicted\n",
    "    \n",
    "    def predict_float(self, features):\n",
    "        \"\"\"\n",
    "        \"Predict\" the un-thresholded value and return it.\n",
    "        \"\"\"\n",
    "        # Break out the feature weights and bias weight for readability\n",
    "        bias_weight = self.weights[0]\n",
    "        feature_weights = self.weights[1:]\n",
    "        # Here we do a dot-product of all the features and their weights\n",
    "        # (sum each feature times its weight) and add the bias weight\n",
    "        predicted = np.dot(features, feature_weights) + bias_weight\n",
    "        return predicted\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Methods\n",
    "Below we add some display methods that allow us to \"look\" at what is going on in the Perceptron. We do this using a trick that allows us to add methods to a class after the class definition. This is normally not the way you would go about it, but I wanted to have the class definition block only contain the essential class definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_errors(self):\n",
    "    plt.figure(num=None, figsize=(8, 3), dpi=96, facecolor=None)\n",
    "    plt.xlim(0, len(self.misclassifications_counter))\n",
    "    plt.ylim(0, np.max(self.misclassifications_counter))\n",
    "    plt.plot(range(1, len(self.misclassifications_counter) + 1), self.misclassifications_counter, marker='o')\n",
    "    plt.xlabel('Iterations (Epochs)')\n",
    "    plt.ylabel('Number of misclassifications')\n",
    "    plt.show()\n",
    "    return self\n",
    "\n",
    "# Add to classs\n",
    "Perceptron.show_errors = show_errors\n",
    "\n",
    "def show(self,show_line=False,resolution=0.1):\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=96, facecolor=None)\n",
    "    plt.scatter(self.features[:50, 0], self.features[:50, 1], color='red', marker='o', label='setosa', alpha=0.5)\n",
    "    plt.scatter(self.features[50:100, 0], self.features[50:100, 1], color='blue', marker='p', label='versicolor', alpha=0.5)\n",
    "    x_min, x_max = self.features[:, 0].min() - 1, self.features[:, 0].max() + 1\n",
    "    y_min, y_max = self.features[:, 1].min() - 1, self.features[:, 1].max() + 1\n",
    "    # Size plot to fit data tightly\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    if show_line:\n",
    "        bias = self.weights[0]\n",
    "        w1 = self.weights[1]\n",
    "        w2 = self.weights[2]\n",
    "        x_intercept = - bias / w1\n",
    "        y_intercept = - bias / w2\n",
    "        slope = - (bias / w2) / (bias / w1)\n",
    "        y_at_x_min = slope * x_min + y_intercept\n",
    "        y_at_x_max = slope * x_max + y_intercept\n",
    "        # We don't need to worry about out-of-bounds values, since pyplot will handle them for us\n",
    "        plt.plot([x_min,x_max],[y_at_x_min,y_at_x_max],'k-')\n",
    "        pass\n",
    "    plt.xlabel('petal length')\n",
    "    plt.ylabel('sepal length')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    return self\n",
    "\n",
    "# Add to class\n",
    "Perceptron.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1005
    },
    "colab_type": "code",
    "id": "7CqICAR1_rNm",
    "outputId": "29c9b621-3247-41ef-b9cf-f6a8fe9e6a09"
   },
   "outputs": [],
   "source": [
    "# Hopefully it looks good at this point. So let's run a trial and plot the\n",
    "# misclassifications each iteration (epoch)\n",
    "ppn = Perceptron()\n",
    "ppn.fit(features, labels)\n",
    "ppn.show()\n",
    "ppn.show_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "tF2fvH2wA3AE",
    "outputId": "83964892-94f6-4ae6-9fe8-dc5e12226747"
   },
   "outputs": [],
   "source": [
    "# If we want to see the decision, we can plot it\n",
    "ppn.show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with random weights\n",
    "ppn.fit(features, labels, init_random=True).show_errors().show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.fit(features, labels, init_random=True).show_errors().show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.fit(features, labels, init_random=True).show_errors().show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.fit(features, labels, init_random=True).show_errors().show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.fit(features, labels, init_random=True).show_errors().show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSCI E-86 Perceptron.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
