{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAUwLWJZx-2K"
   },
   "source": [
    "# A Pedagogical Perceptron Implementation\n",
    "This Perceptron implementation was originally inspired by the the version from \"Python Machine Learning by Sebastian Raschka 1st edition\" (ISBN-13: 978-1783555130), which we used in Spring of 2016. It has subequently been completely re-written in an attempt to make it more pedagogically accessible to less savvy programmers.\n",
    "\n",
    "Please feel free to use this as you see fit, but please give credit (or blame) as appropriate.\n",
    "\n",
    "**Note 1**:check out the newer edition(s) of \"Python Machine Learning\" which is at the second edition as of this typing (and [Sebastian Rascha's website](https://sebastianraschka.com/))as well as other greate books like \"[The Deep Learning Book](https://www.deeplearningbook.org/)\" to support these authors, after all, writing a Noteboook is time consuming, but writing a decent book is exhausting.\n",
    "\n",
    "**Note 2**: This is a WORK IN PROGRESS. I will hopefully be constantly updating and modifying this, but use at your own risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Perceptron\n",
    "Here is an example of a perceptron in few lines of code. In spite of being tiny, it'still able to do things like randomly initialize weights, set them, handle different labels like $[-1.0,1,0]$ or $[1.34, -0.11]$, etc. NB, if you have two lables that are VERY far from $[-1.0, 1.0]$ you'll have to play with the learning rate or else it may never find a boundary.\n",
    "\n",
    "This shows you the power of knowing Python, the Python Packages, and math; however, we are going to go through a longer implementaition step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TinyPerceptron:\n",
    "    def __init__(self, samples, labels, init_val = None):\n",
    "        ulabels = np.unique(labels)\n",
    "        self.a, self.b, self.threshold = ulabels[0], ulabels[1], ulabels.mean()\n",
    "        self.samples = np.insert(samples,0,np.ones(samples.shape[0]),axis=1) # add 1.0 to each sample\n",
    "        self.labels = labels\n",
    "        if init_val is None:\n",
    "            self.weights = np.random.rand(samples.shape[1] + 1)\n",
    "        else:\n",
    "            self.weights = np.full(samples.shape[1] + 1, float(init_val))\n",
    "        pass\n",
    "\n",
    "    def train(self, learning_rate = 0.01, iterations = 10):\n",
    "        for i in range(iterations):\n",
    "            errs = 0\n",
    "            for sample, label in zip(self.samples, self.labels):\n",
    "                error = label - (self.a if np.dot(sample, self.weights) < self.threshold else self.b)\n",
    "                self.weights += learning_rate * error * sample\n",
    "                if error: errs += 1\n",
    "                pass\n",
    "            if errs == 0:\n",
    "                print(\"Found a boundary in %d iterations.\" % (i + 1))\n",
    "                return self\n",
    "            pass\n",
    "        print(\"Failed to find a boundary.\")\n",
    "        pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test the `TinyPerceptron` out, you'll have to first load data and then instantiate and train it. I've provided all you need below.\n",
    "\n",
    "**Note**: Before you can run the `show()` function to visualize the results, you'll have to run the cell in which it is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "labels, samples = np.where(dataset.iloc[0:100, 4].values==0,-1,1), dataset.iloc[0:100, [0, 2]].values\n",
    "\n",
    "tp = TinyPerceptron(samples,labels,0)\n",
    "_ = tp.train()\n",
    "#show(tp,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPXtyXP8NbDx"
   },
   "source": [
    "## Notes about Using this iPython Notebook\n",
    "\n",
    "This was written to run well for students in my class (CSCI E-86 Spring 2016 and later called \"[Building the Brain: A Survey of Artificial Intelligence](https://canvas.harvard.edu/courses/54696)\" at Harvard) and periodically contains references to materials discussed in class as well as instructions to contact me or teaching fellows. Your milage may vary.\n",
    "\n",
    "## Running in Jupyter Notebook\n",
    "\n",
    "This notbook should run on most default Jupyter Notebook installations so long as you have all the requisite Python pachages. Setting up a local Jupyter Notebook for those interested will be covered outside of this Perceptron walk through.\n",
    "\n",
    "\n",
    "## Running this on Google Colab (Recommended Spring 2019 or later)\n",
    "\n",
    "As of Spring 2019, [Google Colab](https://colab.research.google.com/) allows us to use their **free** environment to run this and other notebooks straight from your [Google Drive](https://drive.google.com) or from [GitHub](https://www.github.org). You only need a Google account which all students should have.\n",
    "\n",
    "### Ways to Run on Google Colab\n",
    "\n",
    "**Clone the GitHub repository to Google Drive: Recommended**\n",
    "\n",
    "If you have Git installed on your computer and Google Drive mounted, you can cd into your Google Drive and clone the [CSCI E-86 repository](https://github.com/fariello/cscie86). You may do this from the command-line on most systems with Git installed by doing the following:\n",
    "\n",
    "* Open a command-line (aka terminal)\n",
    "* Change directory (`cd`) into your Google Drive directory. If you've installed in the defaul location then:\n",
    "    * Windows\n",
    "        * When you open a terminal (`cmd`, `command.exe`, `Windows PowerShell` etc.) you should be in your \"home\" directory. The prompt will look something like `C:\\Users\\userna>`.\n",
    "        * Change directory into your Google Drive: `cd Google\\ Drive`\n",
    "    * Mac or Linux:\n",
    "        * Change directory into your Google Drive: `cd ~/Google\\ Drive`\n",
    "* From there, clone the repository\n",
    "    * `git clone https://github.com/fariello/cscie86.git`\n",
    "\n",
    "You may also clone the repository anywhere and upload the directory directly into Google Drive using the web interface.\n",
    "\n",
    "**Floder Location Note**: if you put it somewhere else, you need to keep track of it. All instructions assume that you cloned the repo to this locaiont.\n",
    "\n",
    "**Run Directly From GitHub: Not Recommended**\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/) lets you run straight from a public [GitHub](https://www.github.org) repository. To run this, you should be able to just click on [Perceptron Colab from GitHub](https://colab.research.google.com/github/fariello/cscie86/blob/master/colab/perceptron/perceptron.ipynb).\n",
    "\n",
    "For those of you who want to run all of the walkthroughs and tutorials from the class, you will be able to do the Perceptron this way, but will get a warning when trying to run this directly from my GitHub repository because you are running something that Google did not vet and you did not create. For all you know I, or whomever, could be doing horrible things to your computer, all your data on Google Drive, or Colab, so use this at YOUR OWN RISK. Don't run random things from the Internet just because they look cool.\n",
    "\n",
    "You will be able to save changes you make, but they will be saved to a 'Colab Notebooks' folder that Google will automatically create in the top folder of your Google Drive. Keep that in mind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T01hHHJeD0vx"
   },
   "source": [
    "# Python Packages\n",
    "Python comes with a dizzying number of packages for all sorts of things. If you are going to do data science with python you will likely need to know quite a few. In this implementation, we wil be using:\n",
    "\n",
    "- [Numpy](http://www.numpy.org/): A great mathematical package for n-dimentional arrays and mathy sorts of things.\n",
    "- [MatPlotLib](https://matplotlib.org/): A great matlab-like plotting library for visualization.\n",
    "- [Pandas](https://pandas.pydata.org/): A reduculously easy to use data loading, visualization, processing, etc.\n",
    "- [SKLearn](https://scikit-learn.org/stable/): A package specifically for learning meachine learning.\n",
    "\n",
    "In order to use these packages, they first must be installed on your system. Once installed, we need to \"import them\". Read the comments in the code to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Lu1MhA5cxeG8"
   },
   "outputs": [],
   "source": [
    "# Because we are lazy, I mean efficient, we don't want to type \"numpy\"\n",
    "# when ever we want to use something in the numpy package so we give\n",
    "# it a nickname \"np\" by importing it \"as np\". Thereafter whereever we\n",
    "# would have to type five letters (numpy) we can type two (np) being\n",
    "# 60% more efficient!\n",
    "import numpy as np\n",
    "# We can even give packages inside packages nicknames the same way.\n",
    "# Since we're only going to use the pyplot package inside the\n",
    "# matplotlib package, we can create \"plt\" pointing to it directly.\n",
    "import matplotlib.pyplot as plt\n",
    "# Same for pandas...\n",
    "import pandas as pd\n",
    "# Since we're only going to use the iris data set from sklearn,\n",
    "# we import ony the \"load_iris\" method here. We can now call\n",
    "# it directly\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_DqJSwGD0v4"
   },
   "source": [
    "# Getting the data\n",
    "Here we load the data that we're going to use into a variable called\n",
    "\"`iris`\".\n",
    "\n",
    "The format is one that is a `dict` and has the format:\n",
    "```python\n",
    "  {\n",
    "    'data': array([ [...], [...], ... [...]],\n",
    "    'target': array([0,0,0, ... 2,2,2],\n",
    "    'target_names': array(['setosa', 'versicolor', 'virginica']),\n",
    "    'DESCR': 'Long string description',\n",
    "    'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
    "  }\n",
    "```\n",
    "Note that the `data`, `target`, and `target_names` keys have values that are numpy.nd arrays.\n",
    "This allows us to use all of the great NumPy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Wb5NOEpn7_WF"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# Uncomment to see what this looks like.\n",
    "# print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwupNqCfD0v7"
   },
   "source": [
    "# Converting the data Data Sciency-Style\n",
    "\n",
    "Although the data format of `iris` is useful, we want it in a slightly different format. There are many ways we could use to get to the desired formats, but I'll show you a neat trick to load them into a Pandas data object that allows us to view the data nicely.\n",
    "\n",
    "**NOTE**: You don't need to know the details of this, but it's neat to see what Pandas and NumpPy can do for you in a single lines of code. If you would like an explanation of what is going on or an example of how to do this in a more \"long-hand\" intuitive way, just let me know and we'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "gVMuoXZdD0v8",
    "outputId": "ca94f7a5-9d1a-4600-fab5-f12f856ebee2"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STTpRyE-D0v-"
   },
   "source": [
    "You can probably see why this might be useful, especially to make sure that you have the data you think you have.\n",
    "\n",
    "**Note**: The first column with no heading and just numbers is not really a column in the data, it just shows you the index of each row with the index starting at `0` as in most languages. The data contain 150 rows (fifty for each species) indexed from `0` to `149` and 5 columns index from `0` to `4`.\n",
    "\n",
    "Moving along...\n",
    "\n",
    "# Getting the Samples and Labels (Targets)\n",
    "\n",
    "Ultimately we want a 2-dimensional array of samples, since our Perceptron implementation will only be able to deal with two. The `iris` data set also contain three species of flowers: setosa, versicolor, and virginica. The Perceptron is a binary classifier and therefore can only handle two, so we also need to take only two species.\n",
    "\n",
    "For this exercise, we'll take the first two: setosa and versicolor.\n",
    "\n",
    "We \"know\" that the first 100 rows contain those two species, because we looked at the data (trust me) even though we could use some tricks to extract the first two labels (aka targets). For now, we'll use our knowledge of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Prd9bHagD0v_"
   },
   "source": [
    "## Fancy Array Indexing\n",
    "\n",
    "Here we use \"fancy indexing\" which most array-like objects support. To grab the first 3 rows of a one dimensional array, we can do this:\n",
    "\n",
    "```python\n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(a[0:2])\n",
    "```\n",
    "Which will print `[1,2,3]`. You can also do things like `a[:2]` for the same result, `a[-1]` and much more. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "3LF7s9ZtD0wA",
    "outputId": "25ea3512-3086-492e-88b2-85e3dde4ce1b"
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(\"a = %s\" %a)\n",
    "print(\"a[0:2]  = %s\" %a[0:2])\n",
    "print(\"a[:2]   = %s\" %a[:2])\n",
    "print(\"a[-1]   = %s\" %a[-1])\n",
    "print(\"a[7:]   = %s\" %a[7:])\n",
    "print(\"a[7:10] = %s\" %a[7:10])\n",
    "print(\"a[7:-1] = %s\" %a[7:-1])\n",
    "print(\"a[:]    = %s\" %a[:])\n",
    "print(\"a[::2]  = %s\" %a[::2])\n",
    "print(\"a[1::2] = %s\" %a[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l90qclSwD0wC"
   },
   "source": [
    "NumPy arrays go even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "zFIkmzGqD0wD",
    "outputId": "744c9636-e824-43ea-ea58-09a412692662"
   },
   "outputs": [],
   "source": [
    "a = np.array(a)\n",
    "print(a[a % 3 != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKuJp47KD0wF"
   },
   "source": [
    "## Getting the Labels (Targets)\n",
    "\n",
    "To get the labels (aka targets) from the pandas `dataset` for just the setosa and versicolor species, we'll grab the fith column (index `4`) for the first 100 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eo7XtB249E8l"
   },
   "outputs": [],
   "source": [
    "labels = dataset.iloc[0:100, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVoQD4e6D0wJ"
   },
   "source": [
    "We can see that we did what we thought we did by printing the first 3 and last 3 elements of `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "CAoVkNsWD0wK",
    "outputId": "53dfb535-79c0-4e36-b789-3ab8958e8c64"
   },
   "outputs": [],
   "source": [
    "print(labels[:3],labels[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsZNZqdID0wO"
   },
   "source": [
    "Notice that the labels are `floats`. Remember that the Perceptron does not understand text labels, only numerical ones, so this is what we want.\n",
    "\n",
    "We can also find out how many unique values there are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "4PuPVWzzD0wP",
    "outputId": "a177a792-87ee-47c7-d249-b8fcb6783c2f"
   },
   "outputs": [],
   "source": [
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAEKn-n_D0wS"
   },
   "source": [
    "There are two values only, great! There are many neat things that we can do with NumPy arrays, by the way. If we want to count the number of non-zero values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ve4zkTWPD0wT",
    "outputId": "959b8316-6573-41d0-9d6e-732e1eaac95a"
   },
   "outputs": [],
   "source": [
    "print(np.count_nonzero(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHkj2CghD0wW"
   },
   "source": [
    "If we want to grab all of the rows with zero values, there are several ways to do this, but one great way is to mask the array. You can do that with a simple comparison operator. `labels == 0` will return an array of True and False and if you then use that to index into a NumPy array it will return only those values that have `True`. For example the following resturns the last 50 elements of `labels` since they have a value of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "gWkT2k8PD0wX",
    "outputId": "42b7789b-67e9-4b3f-9e53-f14a5ec3a90d"
   },
   "outputs": [],
   "source": [
    "print(labels[labels == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8SZKUwwD0wZ"
   },
   "source": [
    "Using this method combinbed with the NumPy \"where\" method, we can convert all the values. Since we will first run a Perceptron with classes `-1` and `1` rather than `0` and `1`, we'll change all values accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q0tqs0WaD0wa"
   },
   "outputs": [],
   "source": [
    "labels = np.where(labels == 0, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42IVh4KbD0we"
   },
   "source": [
    "Now setosa is `-1` and versicolor is `1` (unchanged)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLDGLawzD0wf"
   },
   "source": [
    "## Getting the Samples\n",
    "\n",
    "Now we'll get the samples. Our poor little Perceptron binary classifier can only handle two, so we'll pick the sepal length and petal length.\n",
    "\n",
    "**Note**: Don't know what a sepal is? [Look it up](https://en.wikipedia.org/wiki/Sepal)! Don't know what a petal is? See me.\n",
    "\n",
    "Those are found in the first and third columns of our `dataset` (indexes `0` and `2` respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sDfFBYrB-jS_"
   },
   "outputs": [],
   "source": [
    "samples = dataset.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR75gas9D0wj"
   },
   "source": [
    "You may want to make sure you got this right by using `head` and fancy indexing to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Z-qsMOGRD0wk",
    "outputId": "a687ab43-9f41-4908-ed1f-966d16c2fc4d"
   },
   "outputs": [],
   "source": [
    "print(samples[:5])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSlY7xeeD0wo"
   },
   "source": [
    "All good. We could have done this exact same thing in two lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lu-FqylOD0wt"
   },
   "source": [
    "But I think you'll agree that it would have been harder to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cOFpf-mhD0wp"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "labels, samples = np.where(dataset.iloc[0:100, 4].values==0,-1,1), dataset.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzFhMRRaVeuO"
   },
   "source": [
    "## Note about Classes\n",
    "Classes and objects in python are a mostly like classes and object in other OO languages with some\n",
    "annyoing or welcome differences depending on your mood that day. I am going to assume that you\n",
    "understand the concepts of classes and objects well enough to read the code below and understand it\n",
    "especially with all the comments. If you find it confusing, just let me know.\n",
    "I am breaking up the class definition here, not because it's a good idea, because it's not. I do it\n",
    "because I want to add some markup (like this text here) at or near some code.\n",
    "\n",
    "Normally, you would define all the methods in one place under the class:\n",
    "\n",
    "```python\n",
    "class Foo():\n",
    "    def __init__(self,a,b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        pass\n",
    "\n",
    "    def bar(self):\n",
    "        return self.a + self.b\n",
    "    \n",
    "    # [... more stuff ...]\n",
    "    \n",
    "    # End of class\n",
    "    pass\n",
    "```\n",
    "\n",
    "And you would not do cool but crazy stuff like modifying the class afterwards:\n",
    "\n",
    "```python\n",
    "def biz(self, c):\n",
    "    return self.a + self.b * self.c\n",
    "\n",
    "Foo.bing = biz\n",
    "```\n",
    "\n",
    "Which now gives all objects of type `Foo` an instance method called `bing` which is really `biz`:\n",
    "\n",
    "```python\n",
    "f = Foo(1,2)\n",
    "print(f.bing(3)) # prints 1 + 2 * 3 which is 7\n",
    "f.biz(3) # Error\n",
    "biz(3) # Error\n",
    "biz(f,3) # Works, but don't do that.\n",
    "```\n",
    " So please keep in mind that when I use this it is **only** to get around the limitations that you are not\n",
    " allowed to break a class, method, or fuction definition across iPython Notebook cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vztx58frD0wu"
   },
   "source": [
    "# The (Rosenblatt) Perceptron\n",
    "\n",
    "Now for the whole reason we're doing this in the first place.\n",
    "\n",
    "## Background\n",
    "\n",
    "The original Rosenblatt [Perceptron](https://en.wikipedia.org/wiki/Perceptron) was invented by [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) (1928-1971) in 1957 and published in 1958 journal \"Psychological Review\":\n",
    "\n",
    "    Rosenblatt, Frank. \"The perceptron: a probabilistic model for\n",
    "    information storage and organization in the brain.\" Psychological\n",
    "    review 65.6 (1958): 386.\n",
    "\n",
    "The publication itself did not contain the code used, and, as far as we know, the original code if any is not accessible. Lukily the methods in mathematical terms with enough information to implement the origical code.\n",
    "\n",
    "\"[Toward Data Science](https://towardsdatascience.com/)\" has a [nice brief article](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53) by Sagar Sharma explaining the Perceptron further. In case you forgot, the original Perceptron is a *Binary Linear Classifier* which attempts to draw a line (for 2 features), a plane (for 3 features), etc. which creates a decision boundary splitting the data into two regions. Samples with features in one region are labelled one way, all others in the other way.\n",
    "\n",
    "Sebastian Raschka also has [a good, more in-depth explanation](https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#frank-rosenblatts-perceptron) on his site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Class\n",
    "\n",
    "Below we begin the `Perceptron` class. Remember that we split it across cells using Python's ability to add methods to class after they have been declared, but you normally don't want to do that.\n",
    "\n",
    "Typically you document the class and the `__init__` function in the docstring (the string immediately following the `class` declaration. There are several formats that are widely in use, but here we'll try to stick to the [NumPy recommendation](https://numpydoc.readthedocs.io/en/latest/format.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    A reasonably accurate but object-oriented and pythonesque\n",
    "    implementation of the 1957-1958 Rosenblat Perceptron.\n",
    "    \n",
    "    See:\n",
    "    \n",
    "    Rosenblatt, Frank. \"The perceptron: a probabilistic model for\n",
    "    information storage and organization in the brain.\" Psychological\n",
    "    review 65.6 (1958): 386.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        samples : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples\n",
    "            is the number of samples and n_features\n",
    "            is the number of features.\n",
    "        labels : array-like, shape = [n_samples]\n",
    "            Target values.  \n",
    "    \"\"\"\n",
    "    def __init__(self, samples, labels, weight_values = None):\n",
    "        # Store the samples after adding a 1.0 \"hidden\" feature to each\n",
    "        # this is the \"bias value\". Many implementations separate the\n",
    "        # bias weight (calling it just bias), but this way we treat\n",
    "        # all weights the same and simplify the code (see TinyPerceptron)\n",
    "        self.samples = np.insert(samples,0,np.ones(samples.shape[0]), axis=1) # add 1.0 to each sample\n",
    "        # Store the labels\n",
    "        self.labels = labels\n",
    "        # This is a binary classifier, so if there are not two labels, we have a problem.\n",
    "        self.unique_labels = np.unique(labels)\n",
    "        if len(self.unique_labels) != 2:\n",
    "            raise ValueError(\"We need exactly two categories/labels. We received %d.\" %(len(unique_labels)))\n",
    "        self.a_label, self.b_label = self.unique_labels[0], self.unique_labels[1]\n",
    "        # Set the threshold to halfway between the two\n",
    "        self.threshold = self.unique_labels.mean()\n",
    "        if self.samples.shape[0] != labels.shape[0]:\n",
    "            raise ValueError(\"We need there to be as many samples (%d) as there are labels (%d).\"\n",
    "                            %(self.samples.shape[0], labels.shape[0]))\n",
    "        # We want to keep track of the the weights as they progress\n",
    "        self.bias_history = []\n",
    "        self.weights_history = []\n",
    "        self.iteration_count = None\n",
    "        self.num_iterations = None\n",
    "        self.learning_rate = None\n",
    "        self.misclassifications = None\n",
    "        # Initialize the weights\n",
    "        self.initialize_weights(weight_values)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just add a convenience function that allows us to \"see\" the current Perceptron settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_info(self):\n",
    "    print(\"Perceptron:\")\n",
    "    print(\"  Samples: ................ %d\" % self.samples.shape[0])\n",
    "    print(\"  Features per Sample ... : %d\" % (self.samples.shape[1] - 1))\n",
    "    print(\"  Labels: ................. %s\" % self.unique_labels)\n",
    "    print(\"  Threshold: .............. %s\" % self.threshold)\n",
    "    print(\"  Learning Rate (eta): .... %s\" % self.learning_rate)\n",
    "    print(\"  Number of Iterations: ... %s\" % self.num_iterations)\n",
    "    print(\"  Original Bias: .......... %s\" % self.orig_weights[0])\n",
    "    print(\"  Original Weights: ....... %s\" % self.orig_weights[1:])\n",
    "    print(\"  Current Bias: ........... %s\" % self.weights[0])\n",
    "    print(\"  Current Weights: ........ %s\" % self.weights[1:])\n",
    "    print(\"  Iterations Completed: ... %s\" % self.iteration_count)\n",
    "    if self.misclassifications is None:\n",
    "        print(\"  Misclassifications: ..... %s\" % self.misclassifications)\n",
    "    else:\n",
    "        print(\"  Misclassifications: ..... %s\" % np.sum(self.misclassifications))\n",
    "        print(\"  Res: %s\" % self.misclassifications)\n",
    "        pass\n",
    "    return\n",
    "\n",
    "Perceptron.print_info = print_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be doing the same thing many times, it's good to create a function. This convenience function takes an Numpy ndarray and sets all the weights (bias included) based on the values in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_all_weights(self,arr):\n",
    "    \"\"\"\n",
    "    Set all weights (including the bias) using teh np.ndarray arr\n",
    "    \"\"\"\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        raise ValueError(\"We need an np.ndarray object as input not a %s\" %type(arr))\n",
    "        pass\n",
    "    if arr.shape[0] != self.samples.shape[1]:\n",
    "        raise ValueError(\"Array must have %d elements, it had %d (%s)\" %(\n",
    "            self.samples.shape[1],arr.shape[0], arr))\n",
    "        pass\n",
    "    self.weights = arr\n",
    "    self.orig_weights = arr\n",
    "    return self\n",
    "\n",
    "Perceptron.set_all_weights = set_all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method sets the bias weight and the weights for each feature to some random value between -1.0 and 1.0 using some of the neat conveninence tricks afforded to us through NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_random_weights(self):\n",
    "    \"\"\"\n",
    "    Set weights to random values from -1.0 to 1.0\n",
    "    \"\"\"\n",
    "    return self.set_all_weights(np.random.rand(self.samples.shape[1]) * 2 - 1.0)\n",
    "\n",
    "Perceptron.set_random_weights = set_random_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialze all the weights (bias included0 to some values depending on input.\n",
    "* If value is `None`, initialize to random values between -1 and 1.\n",
    "* If value is a NumPy ndarray, set the values to the content of the array.\n",
    "* Otherwise, assume it's a number, convert it to a float, and set all values to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(self,value=None):\n",
    "    \"\"\"\n",
    "    Initialize the bias and weights.\n",
    "    \n",
    "    Paremeters\n",
    "    ----------\n",
    "    value : None or float\n",
    "        If None, all weights will be initialized randomly with values ranging from -1.0 to 1.0\n",
    "        If np.ndarry, set weights with values in the array\n",
    "        Otherwise set all weights to value\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return self.set_random_weights()\n",
    "    # If it's an np.ndarray, assign\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return self.set_all_weights(value)\n",
    "    return self.set_all_weights(np.full(self.samples.shape[1], float(value)))\n",
    "\n",
    "Perceptron.initialize_weights = initialize_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc = Perceptron(samples,labels).initialize_weights()\n",
    "perc.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the predicted value (before thresholding) which is the result of the summation function in class:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=0}^n x_n w_n = ( x_0 w_0 + x_1 w_1 + ... x_n w_n ) = \\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "... \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\bullet\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "... \\\\\n",
    "w_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $x_0$ and $w_0$ are the bias value (1.0) and the bias weight (`self.bias`) respectively. Since the bias value is always 1.0, we don't go around adding `self.bias * 1.0` but just `self.bias`.\n",
    "\n",
    "Since this the same thing as a dot-product (the stuff at the end of the second equals sign) and this could be done using `np.dot` in stead of a loop, which is better. The `np.dot` function is implemented using fast linear algebra libraries that experience a lot of acceleration if there are available GPUs. With two features per sample, this does not matter too much, but keep that in mind.\n",
    "\n",
    "**Remember** we added a `1.0` to each sample, so we can get the results of the dot-product of BOTH the normal weights per feature AND the bais with one `np.dot` call since the features per sample are `[1.0, feature1, feature2]` and the `self.weights` value is `[bias_weight, feature1_weight, feature2_weight]` the tiny extra work done by having to multiply by `1.0` each sample os dwarfed by the ability to use one single `np.dot` call even with only two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_float(self, features):\n",
    "    \"\"\"\n",
    "    \"Predict\" the un-thresholded value and return it as a float.\n",
    "    \"\"\"\n",
    "    return np.dot(features,self.weights)\n",
    "\n",
    "Perceptron.get_predicted_float = get_predicted_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may recall from lecture that artificial neurons will generally have a summation (`get_predicted_float()`) function and a transformation function where the output is transformed usually between the values of 0 and 1 or -1 and 1 in some way, sometimes using a sigmoidal function or a rectifier. The transformation function used by the Perceptron is a simple thresholding (aka step) function:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\begin{cases}\n",
    "1\\ if\\ \\geq \\theta \\\\\n",
    "0\\ if\\ \\lt \\theta\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "OR\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\begin{cases}\n",
    "1\\ if\\ \\geq \\theta \\\\\n",
    "-1\\ if\\ \\lt \\theta\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Where theta ($\\theta$) is the threshold.\n",
    "\n",
    "Since we know that `self.a_label` and `self.b_label` contain the two values we set the threshold `self.threshold` to halfway between the two in the initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(self, features):\n",
    "    \"\"\"\n",
    "    \"Predict\" the estimated category as a continuous float.\n",
    "    \"\"\"\n",
    "    # Get the float predicted label and theshold it to make this binary\n",
    "    return self.b_label if self.get_predicted_float(features) < self.threshold else self.a_label\n",
    "\n",
    "Perceptron.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method that does one iteration over the training samples and tries to improve the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_once(self):\n",
    "    # Number of errors encountered (misclassifications)\n",
    "    error_count = 0\n",
    "    # Iterate over each sample in the training set\n",
    "    for features, correct_label in zip(self.samples, self.labels):\n",
    "\n",
    "        # Based on the current weights and bias, what would we predict\n",
    "        # this sample to be?\n",
    "        predicted_label = self.predict(features)\n",
    "        # The difference between the correct and predicted\n",
    "        # tells us how far off we were. This could be positive\n",
    "        # or negative. Note that which you subtract from which is\n",
    "        # very important since the sign matters.\n",
    "        error_amount = predicted_label - correct_label\n",
    "\n",
    "        # If there was a difference, nudge the weights.\n",
    "        if error_amount != 0:\n",
    "\n",
    "            # Now \"nudge\" the weights by the error amount multiplied\n",
    "            # by the learning rate for each feature.\n",
    "            self.weights +=  self.learning_rate * error_amount * features\n",
    "\n",
    "            # Since we were wrong, count the error\n",
    "            error_count += 1\n",
    "            \n",
    "            pass\n",
    "        pass\n",
    "    return error_count\n",
    "\n",
    "Perceptron.train_once = train_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, learning_rate=0.01, num_iterations=50):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_iterations = num_iterations\n",
    "    self.iteration_count = 0\n",
    "    self.misclassifications = []\n",
    "    for _ in range(self.num_iterations):\n",
    "        self.iteration_count += 1\n",
    "        misclassifications = self.train_once()\n",
    "        self.misclassifications.append(misclassifications)\n",
    "        if misclassifications <= 0.0:\n",
    "            return self\n",
    "        pass\n",
    "    return self\n",
    " \n",
    "Perceptron.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0iGp4z_D0wx"
   },
   "source": [
    "# Display Methods\n",
    "Below we add some display methods that allow us to \"look\" at what is going on in the Perceptron. We do this using a trick that allows us to add methods to a class after the class definition. This is normally not the way you would go about it, but I wanted to have the class definition block only contain the essential class definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BGbyo69HD0wy"
   },
   "outputs": [],
   "source": [
    "def show_errors(self):\n",
    "    # Create a decent sized figure, but half the hight of the \"show\" figure\n",
    "    plt.figure(num=None, figsize=(8, 3), dpi=96, facecolor=None)\n",
    "    # Set the x axis limits\n",
    "    plt.xlim(0, len(self.misclassifications))\n",
    "    # Set the y axis limites to one more than then max value so that it\n",
    "    # does not look cut-off\n",
    "    plt.ylim(0, np.max(self.misclassifications) + 1)\n",
    "    # We really only want integer tick marks\n",
    "    plt.yticks(range(np.max(self.misclassifications) + 1))\n",
    "    plt.xticks(range(len(self.misclassifications)))\n",
    "    # Titles are nice.\n",
    "    plt.title(\"Misclassifications per Iteration (Epoch)\")\n",
    "    plt.plot(range(1, len(self.misclassifications) + 1), self.misclassifications, marker='o')\n",
    "    plt.xlabel('Iterations (Epochs)')\n",
    "    plt.ylabel('Number of misclassifications')\n",
    "    plt.show()\n",
    "    return self\n",
    "\n",
    "# Add to classs\n",
    "Perceptron.show_errors = show_errors\n",
    "\n",
    "def show(self,show_decision_boundary=False,resolution=0.1):\n",
    "    # Create a decent sized figure\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=96, facecolor=None)\n",
    "    # For showing, we have to pull out the actual features\n",
    "    samples = self.samples[:,1:3]\n",
    "    # Plot the feature samples. We use transparency because some of the samples\n",
    "    # have identical samples and it's nice to see when there are two or more\n",
    "    # \"dots\" on the plot.\n",
    "    plt.scatter(samples[:50, 0], samples[:50, 1], color='red', marker='o', label='setosa', alpha=0.33)\n",
    "    plt.scatter(samples[50:100, 0], samples[50:100, 1], color='blue', marker='p', label='versicolor', alpha=0.33)\n",
    "    # Calculate the max and min making them 0.1 wider for the plot so the\n",
    "    # markers fit visibly.\n",
    "    x_min, x_max = samples[:, 0].min() - 0.1, samples[:, 0].max() + 0.1\n",
    "    y_min, y_max = samples[:, 1].min() - 0.1, samples[:, 1].max() + 0.1\n",
    "    # Size plot to fit data tightly\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    if show_decision_boundary:\n",
    "        # If we're plotting the decision boundary, it would be nice to say so\n",
    "        plt.title(\"Iris Data with Decision Boundary\")\n",
    "        # Calculate the line that is the decision boundary. You may recall from\n",
    "        # algebra the eqution y = mx+b where m is the slope and b is the\n",
    "        # x intercept. We can calculate all of these using the weights and bias\n",
    "        bias = self.weights[0]\n",
    "        w1 = self.weights[1]\n",
    "        w2 = self.weights[2]\n",
    "        x_intercept = - bias / w1\n",
    "        # We don't really need the y_intercept, but if you were wondering, here\n",
    "        # it is.\n",
    "        y_intercept = - bias / w2\n",
    "        # And here is the slope\n",
    "        slope = - (bias / w2) / (bias / w1)\n",
    "        # Now we can calculate y at the minimum x-axis value (from above) using\n",
    "        # the good-old y = mx + b formula\n",
    "        y_at_x_min = slope * x_min + y_intercept\n",
    "        # And the y value at the maximum x-axis value\n",
    "        y_at_x_max = slope * x_max + y_intercept\n",
    "        # Plotting a line from x_min, y_at_x_min to x_max, y_at_x_max will\n",
    "        # draw the line from one end of the plot to the other that shows\n",
    "        # the decision boundary at the time that this method was called.\n",
    "        plt.plot([x_min,x_max],[y_at_x_min,y_at_x_max],'k-',label='decision boundary', alpha=0.5)\n",
    "    else:\n",
    "        plt.title(\"Iris Data\")\n",
    "        pass\n",
    "    plt.xlabel('petal length')\n",
    "    plt.ylabel('sepal length')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    return self\n",
    "\n",
    "# Add to class\n",
    "Perceptron.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "7CqICAR1_rNm",
    "outputId": "e56a29f0-4079-4eb1-8b2c-5dcfd7612c24"
   },
   "outputs": [],
   "source": [
    "# Hopefully it looks good at this point. So let's run a trial and plot the\n",
    "# misclassifications each iteration (epoch)\n",
    "perc = Perceptron(samples,labels, 0.0)\n",
    "perc.print_info()\n",
    "perc.train()\n",
    "perc.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tF2fvH2wA3AE",
    "outputId": "bf460065-8f17-43a2-e697-a768888e7232"
   },
   "outputs": [],
   "source": [
    "perc.show_errors()\n",
    "# If we want to see the decision, we can plot it\n",
    "_ = perc.show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ReWNBHV5D0w4",
    "outputId": "b5ccbac9-a3f7-497a-fd01-9a47fea8a9a3"
   },
   "outputs": [],
   "source": [
    "# Now let's try with random weights\n",
    "_ = perc.initialize_weights().train().show_errors().show(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSCI E-86 Perceptron.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
